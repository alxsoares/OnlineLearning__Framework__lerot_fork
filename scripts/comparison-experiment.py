#!/usr/bin/python

# KH, 2012/09/12
# Runs a comparison experiment as single query run (queries are provided one
# per file in a query directory).

import logging
import argparse
import gzip
import os
import os.path
import sys
import yaml

import numpy as np
from numpy.linalg import norm

try:
    from include import *
except:
    raise

from lerot.query import load_queries
from lerot.utils import get_class

from random import choice
from traceback import format_exc

def run(run_count, experimenter, args):
    logging.info("run %d starts" % run_count)
    # initialize log file
    try:
        log_file = os.path.join(args["output_dir"], "%s-%d.txt.gz" % (
            args["output_prefix"], run_count))
        log_fh = gzip.open(log_file, "wb")
        # initialize experiment
        experiment = experimenter(args["query_dir"], args["feature_count"], log_fh,
                                  args)
        # run experiment
        experiment.run()
    except:
        logging.error(format_exc())
    finally:
        # clean up
        log_fh.close()
        logging.info("run %d done" % run_count)


# initialize and run a learning experiment
if __name__ == "__main__":
    # parse arguments
    parser = argparse.ArgumentParser(
        prog="python evaluation-experiment.py",
        description="""
        Construct and run a comparison experiment with live or historical data.
        Provide either the name of a config file from which the experiment
        configuration is read, or provide all arguments listed under Command
        line. If both are provided the config file is ignored.""",
        usage="%(prog)s FILE | DETAILS")

    # option 1: use a config file
    file_group = parser.add_argument_group("FILE")
    file_group.add_argument("-f", "--file", help="Filename of the config file "
        "from which the experiment details should be read.")

    # option 2: specify all experiment details as arguments
    detail_group = parser.add_argument_group("DETAILS")
    detail_group.add_argument("-query_dir", help="Directory with training "
        "query files (one per query, svmlight format).")
    detail_group.add_argument("--feature_count", type=int,
        help="The number of features included in the data.")
    detail_group.add_argument("--num_runs", type=int,
        help="Number of runs (how many times to repeat the experiment).")
    detail_group.add_argument("--run_start_id", type=int, default=0,
        help="Starting id for numbering run files.")
    detail_group.add_argument("--processes", type=int, help="Number of process"
        "es if the experiments are to be run in parallel.")
    detail_group.add_argument("--num_queries", type=int,
        help="Number of queries in each run.")
    detail_group.add_argument("--result_length", type=int, default=10,
        help="Length of the result lists to show to users for each query.")
    detail_group.add_argument("--check_queries", type=bool,
        help="Check to only use queries with at least two relevance grades.")
    detail_group.add_argument("--check_rankers", type=bool,
        help="Check rankers and only use ranker pairs with a non-zero ndcg "
        "difference")
    detail_group.add_argument("--user_model",
        help="Class implementing a user model.")
    detail_group.add_argument("--user_model_args",
        help="Arguments for initializing the user model.")
    detail_group.add_argument("--live_evaluation_methods", nargs="*",
        help="List of zero or more live evaluation methods to run.")
    detail_group.add_argument("--live_evaluation_methods_args", nargs="*",
        help="Arguments for the live evaluation methods (one entry per method,"
        " in the same order).")
    detail_group.add_argument("--hist_evaluation_methods", nargs="*",
        help="List of zero or more evaluation methods for historical data.")
    detail_group.add_argument("--hist_evaluation_methods_args", nargs="*",
        help="Arguments for the historical evaluation methods.")
    # the retrieval system maintains ranking functions, accepts queries and
    # generates result lists, and in return receives user clicks to learn from
    detail_group.add_argument("--output_dir",
        help="(Empty) directory for storing output generated by this"
        " experiment. Subdirectory for different folds will be generated"
        "automatically.")
    detail_group.add_argument("--output_prefix",
        help="Prefix to be added to output filenames, e.g., the name of the "
        "data set, fold, etc. Output files will be stored as OUTPUT_DIR/"
        "PREFIX-RUN_ID.txt.gz")
    detail_group.add_argument("--output_dir_overwrite", type=bool,
        help="Set to true to overwrite existing output directories. False by "
        "default to prevent accidentally deleting previous results.")
    detail_group.add_argument("--experimenter",
        help="Experimenter type.")
    # run the parser
    args = parser.parse_known_args()[0]

    # determine whether to use config file or detailed args
    experiment_args = None
    if args.file:
        config_file = open(args.file)
        experiment_args = yaml.load(config_file)
        config_file.close()
        # overwrite with command-line options if given
        for arg, value in vars(args).items():
            if value:
                experiment_args[arg] = value
    else:
        experiment_args = vars(args)

    # workaround - check if we have all the arguments needed
    if not ("query_dir" in experiment_args and
            experiment_args["query_dir"] and
            "feature_count" in experiment_args and
            experiment_args["feature_count"] and
            "num_runs" in experiment_args and
            experiment_args["num_runs"] and
            "num_queries" in experiment_args and
            experiment_args["num_queries"] and
            "user_model" in experiment_args and 
            experiment_args["user_model"] and
            "user_model_args" in experiment_args and
            experiment_args["user_model_args"] and
            "output_dir" in experiment_args and
            experiment_args["output_dir"]):
        parser.print_help()
        sys.exit("Missing required arguments, please check the program"
                 " arguments or configuration file.")

    # set default values for optional arguments
    if not "run_start_id" in experiment_args:
        experiment_args["run_start_id"] = 0
    if not "output_dir_overwrite" in experiment_args:
        experiment_args["output_dir_overwrite"] = False
    if not "result_length" in experiment_args:
        experiment_args["result_length"] = 10
    if not "experimenter" in experiment_args:
        experiment_args["experimenter"] = \
            "experiment.SingleQueryComparisonExperiment"

    # check arguments
    valid_args = vars(args).keys()
    for key in experiment_args:
        if not key in valid_args:
            raise ValueError("Unknown argument: %s, run with -h for details."
                % key)

    #logging.basicConfig(filename=os.path.join(experiment_args["output_dir"], 
    #'experiment.log'), level=logging.INFO)
    logging.basicConfig(format='%(asctime)s %(module)s: %(message)s', 
                        level=logging.INFO)

    logging.info("Arguments: %s" % experiment_args)

    # locate or create directory for the current fold
    if not os.path.exists(experiment_args["output_dir"]):
        os.makedirs(experiment_args["output_dir"])
    elif not(experiment_args["output_dir_overwrite"]) and \
                                    os.listdir(experiment_args["output_dir"]):
        # make sure the output directory is empty
        raise Exception("Output dir %s is not an empty directory. "
        "Please use a different directory, or move contents out of the way." %
         experiment_args["output_dir"])
    config_bk = os.path.join(experiment_args["output_dir"], "config_bk.yml")
    logging.info("Backing up configuration to: %s" % config_bk)
    config_bk_file = open(config_bk, "w")
    yaml.dump(experiment_args, config_bk_file, default_flow_style=False)
    config_bk_file.close()

    # load query
    logging.info("Checking query directory: %s " %
        experiment_args["query_dir"])
    if not os.path.exists(experiment_args["query_dir"]):
        raise ValueError("query_dir does not exist: %s" %
            experiment_args["query_dir"])
    logging.info("... found %d queries." %
        len(os.listdir(experiment_args["query_dir"])))

    # initialize and run the experiment num_run times
    num_runs = experiment_args["num_runs"]
    run_start_id = experiment_args["run_start_id"]
    experimenter = get_class(experiment_args["experimenter"])

    if "processes" in experiment_args and experiment_args["processes"] > 1:
        from multiprocessing import Pool
        pool = Pool(processes=experiment_args["processes"])
        for run_count in range(run_start_id, run_start_id + num_runs):
            pool.apply_async(run, (run_count, experimenter, experiment_args,))
        pool.close()
        pool.join()
    else:
        for run_count in range(run_start_id, run_start_id + num_runs):
            run(run_count, experimenter, experiment_args)
